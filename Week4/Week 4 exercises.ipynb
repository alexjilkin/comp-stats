{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "University of Helsinki, Master's Programme in Mathematics and Statistics  \n",
    "MAST32001 Computational Statistics, Autumn 2023  \n",
    "Luigi Acerbi\n",
    "\n",
    "# Week 4 exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MCMC convergence diagnostics (4 pts)\n",
    "\n",
    "The file loaded below contains samples from 3 chains of Metropolis-Hastings sampling for the bimodal target \n",
    "$$\\pi^*(\\theta) = \\exp(-\\gamma (\\theta^2-1)^2)$$\n",
    "with $\\gamma = 4$. Warm-up samples have already been removed from the saved results, so you do not need to remove warm-up samples.\n",
    "\n",
    "1. Evaluate the $\\hat{R}$-statistic defined in Sec. 8.1.1 of the course notes using the original 3 chains. Report your result in Moodle.\n",
    "2. Evaluate the split-$\\hat{R}$ (see Sec. 8.1.1) by splitting the chains in half to obtain 6 chains. Report your result in Moodle.\n",
    "3. Compute the effective sample size (ESS) for the first chain (1000 samples), when we know the estimates $\\mu = 0$, $\\sigma^2 = 0.9185$ from an independent long simulation. Use the truncated sum estimate from Sec. 8.1.2 of the course notes. Report your result in Moodle.\n",
    "   *Hint*:  Use Eqs. 8.1 and 8.4, for which you will have to find the upper bound $K$ of the sum as indicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) R_roof=1.025898298169984\n",
      "(2) Split R_roof=1.1155720873796102\n",
      "87\n",
      "(3) ESS=13.04416694301861\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "samples = pd.read_csv('https://raw.githubusercontent.com/lacerbi/compstats-files/main/data/bimod_samples.txt', header=None, sep='\\t').values\n",
    "\n",
    "def R_roof(chains: list[list[int]]):\n",
    "    n = len(chains[0])\n",
    "    m = len(chains)\n",
    "\n",
    "    theta_dot_dot = (1/m) * np.sum([np.mean(chains[j]) for j in range(0, m)])\n",
    "\n",
    "    B = (n/(m-1)) * np.sum([(np.mean(chains[j]) - theta_dot_dot)**2 for j in range(0, m)])\n",
    "    W = (1/(m*(n-1))) * np.sum([(chains[j] - np.mean(chains[j]))**2 for j in range(0, m)])\n",
    "    var_theta = ((n - 1) / n)*W + B/n\n",
    "\n",
    "    return np.sqrt(var_theta/W)\n",
    "\n",
    "chains = [samples[:,0], samples[:,1], samples[:,2]]\n",
    "print(f'(1) R_roof={R_roof(chains)}')\n",
    "\n",
    "half_chains = [samples[:500,0], samples[500:,0], samples[:500,1], samples[500:,1], samples[:500,2], samples[500:,2]]\n",
    "print(f'(2) Split R_roof={R_roof(half_chains)}')\n",
    "\n",
    "##### 2 ######\n",
    "mu = 0\n",
    "sigma_sq = 0.9185\n",
    "chain = chains[0]\n",
    "\n",
    "def ro(k, chain):\n",
    "    sum = 0\n",
    "    n = len(chain) - k\n",
    "\n",
    "    for i in range(n):\n",
    "        sum += ((chain[i] - mu)*(chain[i+k] - mu))\n",
    "\n",
    "    return (sum / n) / sigma_sq\n",
    "\n",
    "K = 0\n",
    "j = 1\n",
    "while (ro(j, chain) + ro(j+1, chain) >= 0):\n",
    "    K = j\n",
    "    j += 2\n",
    "K += 2\n",
    "print(K)\n",
    "M = len(chain)\n",
    "ESS = M / (1 + 2*np.sum([ro(k, chains[0]) for k in range(K+1)]))\n",
    "print(f'(3) ESS={ESS}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MCMC sampling for a Bayesian regression model (6 pts)\n",
    "\n",
    "In this task we will apply MCMC sampling to a linear regression model that predicts the birth weight ('bwt', $y_i$) for old mothers (age >= 30) as a function of the age of the mother, duration of the pregnancy and weight of the mother ('age', 'gestation', 'weight'; $\\mathbf{x}_i$).\n",
    "\n",
    "As our model we will consider a linear regression model fitted to the observations $((\\mathbf{x}_1, y_1), (\\mathbf{x}_2, y_2), \\dots, (\\mathbf{x}_n, y_n))$. The code below will load the matrix `x` (each row is one sample) and vector `y`. The values $\\mathbf{x}_1, \\dots, \\mathbf{x}_n$ are the *input values* and $y_1, \\dots, y_n$ are the *target values*. The loading code normalises all data to have zero mean and unit variance to get the regression coefficients to a shared scale.\n",
    "\n",
    "We model the data with a linear regression model\n",
    "$$ y_i = \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, $$\n",
    "where $\\mathbf{x}_i$ is a row vector of input values, $\\boldsymbol{\\beta} = (\\beta_1, \\beta_2, \\beta_3)^T$ is a column vector of regression coefficients and $\\epsilon_i$ is a Gaussian noise term $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_x^2)$.\n",
    "\n",
    "The likelihood of the model is\n",
    "$$ p(Y \\mid X, \\beta, \\sigma_x) = \\prod_{i=1}^n \\mathcal{N}(y_i; \\mathbf{x}_i \\boldsymbol{\\beta}, \\sigma_x^2). $$\n",
    "\n",
    "The regression coefficients $\\beta_j$ have a hierarchical prior\n",
    "$$ p(\\beta_j | \\sigma_{\\beta}) = \\mathcal{N}(\\beta_j;\\; 0, \\sigma_{\\beta}^2), \\quad j = 1, 2, 3, $$\n",
    "with standard deviation $\\sigma_{\\beta}$.\n",
    "Its prior is\n",
    "$$ p(\\sigma_{\\beta}) = \\mathrm{Gamma}(k_{\\beta} = 2, \\theta_{\\beta} = 1/2) $$\n",
    "with shape $k_{\\beta}=2$ and scale $\\theta_{\\beta} = 1/2$.\n",
    "The prior of the noise standard deviation $\\sigma_x$ is\n",
    "$$ p(\\sigma_{x}) = \\mathrm{Gamma}(k_{x} = 2, \\theta_{x} = 1/2) $$\n",
    "with shape $k_{x}=2$ and scale $\\theta_{x} = 1/2$.\n",
    "As usual, we assume the prior factorizes as $p(\\sigma_\\beta, \\sigma_x) = p(\\sigma_\\beta) p(\\sigma_x)$.\n",
    "\n",
    "To ensure the standard deviations are positive, express them as $\\sigma_{\\beta} = \\exp(s_{\\beta})$ and $\\sigma_x = \\exp(s_x)$. Remember to apply the density transformation to transform the priors over $\\sigma_{\\beta}$ and $\\sigma_x$ to those over $s_{\\beta}$ and $s_x$!\n",
    "\n",
    "The full set of model parameters for MCMC is thus $\\theta = (s_{\\beta}, s_x, \\beta_1, \\beta_2, \\beta_3)$.\n",
    "\n",
    "Write a Metropolis-Hastings MCMC sampler to sample from the joint posterior distribution of all model parameters $\\theta$, $$p(\\theta \\mid X, Y) = p(s_{\\beta}, s_x, \\beta_1, \\beta_2, \\beta_3 \\mid X, Y).$$\n",
    "\n",
    "1. Implement a Metropolis-Hastings MCMC sampler with proposal $q(\\theta' ; \\theta) = \\mathcal{N}(\\theta';\\; \\theta, 0.04^2 \\mathbf{I})$, i.e. a multivariate normal proposal centred around the current state with standard deviation 0.04 for each component.\n",
    "2. Run the sampler for 5000 iterations starting from each of the 4 initial points defined by the columns of `inits`. Discard the first 2500 iterations for each run as warm-up. Compute the split-$\\hat{R}$ statistic for the samples of $\\theta$. Report the largest split-$\\hat{R}$ over different components to Moodle.\n",
    "3. Check which variable is having convergence issues. Increase the proposal standard deviation for that component to 0.4 while keeping the proposals for the other components the same. Repeat the sampling as above and report the largest split-$\\hat{R}$ over different components to Moodle.\n",
    "4. Report the posterior mean of the log-standard deviation $s_{\\beta}$ obtained from combining all the 10000 retained samples from the latter sampler to Moodle.\n",
    "\n",
    "*Notes*: \n",
    "- Please note that all computations and results are based on the unbounded variables $s_{\\beta}$ and $s_x$. For the purpose of this exercise, there is no need to transform anything back to the original space of $\\sigma_{\\beta}$ and $\\sigma_x$.\n",
    "- The [Gamma distribution](https://en.wikipedia.org/wiki/Gamma_distribution) has two common representations, one in terms of shape and scale parameters ($k$ and $\\theta$) and one in terms of shape and rate parameters ($\\alpha$ and $\\beta$). Please check that you are using the correct one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For run #0 Largest split R_hatdor1.0401458486032669\n",
      "[1.0401458486032669, 1.0016738794696292, 1.0215633735252125, 1.0192199026840087, 1.0260489407969653]\n",
      "For run #1 Largest split R_hatdor1.280500306696779\n",
      "[1.280500306696779, 1.0095922298438273, 1.0030065577035818, 1.016573191348837, 1.0199503809948494]\n",
      "For run #2 Largest split R_hatdor1.0409353474023757\n",
      "[1.0409353474023757, 1.0044606367021454, 1.0052688694653225, 1.0033347268841275, 1.0125310476136904]\n",
      "For run #3 Largest split R_hatdor1.036955147858673\n",
      "[1.0018419030284185, 1.0250153987461739, 1.0349727943871203, 1.036955147858673, 1.0310491997245594]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "from scipy import stats\n",
    "\n",
    "# Load the data set\n",
    "babies_full = pd.read_csv(\"https://raw.githubusercontent.com/lacerbi/compstats-files/main/data/babies2.txt\", sep='\\t')\n",
    "\n",
    "# Pick a subset\n",
    "babies = babies_full.loc[(babies_full['age']>=30).values,:]\n",
    "\n",
    "x = babies[['age', 'gestation', 'weight']].values.astype(float)\n",
    "y = babies[['bwt']].values.astype(float).squeeze()\n",
    "\n",
    "# Remove mean from inputs\n",
    "x -= np.mean(x, 0)\n",
    "# Standardise input variance\n",
    "x /= np.std(x, 0)\n",
    "\n",
    "# Remove mean from outputs\n",
    "y -= np.mean(y, 0)\n",
    "# Standardise output variance\n",
    "y /= np.std(y, 0)\n",
    "\n",
    "inits = np.array([[ 1.67272789, -0.02134183, -0.78116796, -1.77420787],\n",
    "                  [-0.6030697,   0.4464263,   0.3627991,  -0.7342916 ],\n",
    "                  [-1.32685026, -0.37427079, -0.06744599,  0.21175491],\n",
    "                  [ 0.87702047, -0.54171875, -0.44736686, -2.39045985],\n",
    "                  [ 1.00631791, -1.34638203, -0.40343913, -0.64535026]])\n",
    "\n",
    "def prior(beta, s_beta, s_x):\n",
    "    beta_prior = np.sum(stats.norm.logpdf(beta, loc=0, scale=np.exp(s_beta)))\n",
    "    \n",
    "    log_sigma_beta_prior = stats.gamma.logpdf(np.exp(s_beta), a=2, scale=1/2) + s_beta\n",
    "    log_sigma_x_prior = stats.gamma.logpdf(np.exp(s_x), a=2, scale=1/2) + s_x\n",
    "    \n",
    "    return beta_prior + log_sigma_beta_prior + log_sigma_x_prior\n",
    "\n",
    "def likelihood(y, x, beta, sigma_x):\n",
    "    return np.sum(stats.norm.logpdf(y, loc=x.dot(beta), scale=np.exp(sigma_x)))\n",
    "\n",
    "def posterior(y, x, theta):\n",
    "    s_beta, s_x, beta1, beta2, beta3 = theta\n",
    "    beta = np.array([beta1, beta2, beta3])\n",
    "    return likelihood(y, x, beta, s_x) + prior(beta, s_beta, s_x)\n",
    "\n",
    "def mcmc(y, x, init, iterations, proposal_std):\n",
    "    n_params = len(init)\n",
    "    samples = []\n",
    "    current_theta = init\n",
    "    current_log_prob = posterior(y, x, current_theta)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        proposed_theta = current_theta + proposal_std * np.random.randn(n_params)\n",
    "        proposed_log_prob = posterior(y, x, proposed_theta)\n",
    "\n",
    "        if proposed_log_prob > current_log_prob or np.random.rand() < np.exp(proposed_log_prob - current_log_prob):\n",
    "            current_theta = proposed_theta\n",
    "            current_log_prob = proposed_log_prob\n",
    "\n",
    "        samples.append(current_theta)\n",
    "\n",
    "    return samples\n",
    "\n",
    "def R_roof(chains: list[list[int]]):\n",
    "    n = len(chains[0])\n",
    "    m = len(chains)\n",
    "\n",
    "    theta_dot_dot = (1/m) * np.sum([np.mean(chains[j]) for j in range(0, m)])\n",
    "\n",
    "    B = (n/(m-1)) * np.sum([(np.mean(chains[j]) - theta_dot_dot)**2 for j in range(0, m)])\n",
    "    W = (1/(m*(n-1))) * np.sum([(chains[j] - np.mean(chains[j]))**2 for j in range(0, m)])\n",
    "    var_theta = ((n - 1) / n)*W + B/n\n",
    "\n",
    "    return np.sqrt(var_theta/W)\n",
    "\n",
    "N = 5000\n",
    "M = N // 2\n",
    "proposal_std = 0.04\n",
    "all_samples = np.array([mcmc(y, x, init, N, proposal_std) for init in inits.T])\n",
    "\n",
    "for sampler_index in range(0, len(all_samples)):\n",
    "    rhat_values = []\n",
    "\n",
    "    for i in range(len(all_samples[sampler_index][0])):\n",
    "        values = all_samples[sampler_index][:, i]\n",
    "\n",
    "        value = R_roof(np.array([values[0: len(values)//2], values[len(values)//2:]]))\n",
    "        rhat_values.append(value)\n",
    "    largest_rhat = np.max(rhat_values)\n",
    "    print(f\"For run #{sampler_index} Largest split R_hatdor{largest_rhat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Importance sampling (4 pts)\n",
    "\n",
    "Develop an importance sampler for the bimodal target\n",
    "$$\\pi(\\theta) = \\sum_{i=1}^2 p_i \\mathcal{N}(\\theta ; \\mu_i, \\sigma_i^2),$$\n",
    "where $p_1 = p_2 = 1/2$ and $\\mu_1 = -1, \\mu_2 = 1$.\n",
    "Use $\\mathrm{Laplace}(0, b)$ with a suitable $b$ as the proposal and evaluate the expectation $\\mathbb{E}[(\\theta-1)^2]$ when\n",
    "1. $\\sigma_1^2 = \\sigma_2^2 = 0.5$.\n",
    "2. $\\sigma_1^2 = \\sigma_2^2 = 0.1$.\n",
    "\n",
    "The required tolerance for the answer is $\\pm 0.1$.\n",
    "\n",
    "In order to estimate the accuracy of your answer, it is recommended to run the sampler a few times and compute the standard deviation of the values you obtain. Monte Carlo error scales as $1/\\sqrt{n}$ with the number of iterations $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Rejection ABC for a dynamical model (6 pts)\n",
    "\n",
    "In this task we will develop an ABC sampler for the autoregressive (AR) model:\n",
    "$$ x_{t+1} = a x_t + \\epsilon_t, \\quad \\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2). $$\n",
    "Throughout the exercise, we assume $x_0 = 1$. The model has two parameters, $a$ and $\\sigma$. We set priors\n",
    "$$ p(a) = \\mathrm{Uniform}(a;\\; 0, 1), \\quad p(\\sigma) = \\mathrm{Gamma}(\\sigma;\\; k_\\sigma, \\theta_\\sigma) $$\n",
    "with $k_\\sigma = 8, \\theta_\\sigma = 1/8$. Note that we use the shape/scale parametrisation also used by NumPy and that the prior is over $\\sigma$, not $\\sigma^2$ (also more consistent with NumPy parametrisation).\n",
    "\n",
    "1. Implement a function to simulate the AR process. Test your function by generating and plotting two independent realisations of length 200 with $a = 0.75$, $\\sigma = 0.2$. Notice how the sequences diverge relatively quickly and are essentially independent toward the end. *Hint*: Length 200 means that the generated sequence will take values $x_0, x_1, \\ldots, x_{199}$.\n",
    "2. Sample 5000 sequences from the AR process of length 200 with $a = 0.75$, $\\sigma = 0.2$. For each sequence, extract the last value. Compute the standard deviation of the last values across the 5000 simulated sequences and report it in Moodle.\n",
    "3. For the purpose of ABC sampling, we are going to summarize the sequences with the 2-dimensional summary statistics\n",
    "$$ S(X) = \\left( \\frac{1}{N} \\sum_{i=0}^{N-1} x_i^2, \\frac{1}{N-1} \\sum_{i=0}^{N-2} x_i x_{i+1} \\right). $$\n",
    "Load the the single observed sequence $X$ (the data) with the code below and evaluate $S(X)$ on the provided sequence. Report the two summary statistics of the data in Moodle.\n",
    "4. Implement an ABC sampler to infer the posterior over $a$ and $\\sigma$ given the single observed sequence $X$ loaded below using the simulator implemented above and the 2-dimensional summary statistics defined above. Run your sampler to generate samples with acceptance threshold $\\| S(X) - S(X^\\star)\\|_2 \\le \\epsilon$ with $\\epsilon = 0.2$.\n",
    "5. Report the approximate posterior means and standard deviations of $a$ and $\\sigma$ to Moodle.\n",
    "\n",
    "The required tolerance on the posterior means and standard deviations is $\\pm 0.03$.\n",
    "\n",
    "*Hints*:\n",
    "- Note that for the acceptance threshold we are using the standard Euclidean metric: $\\|v\\|_2 = \\sqrt{\\sum_{i} v_i^2}$.\n",
    "- The ABC sampler here can easily require tens of seconds or more when the number of samples is large, so you should be careful when increasing the number of samples. A reasonably good implementation should be able to reach the required accuracy in less than a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataframe = pd.read_csv('https://raw.githubusercontent.com/lacerbi/compstats-files/main/data/ar_time_series_data.txt', header=None, sep='\\t')\n",
    "data = dataframe.values[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
